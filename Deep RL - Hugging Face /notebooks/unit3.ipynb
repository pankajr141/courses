{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pankajr141/courses/blob/main/Deep%20RL%20-%20Hugging%20Face%20/notebooks/unit3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7xBVPzoXxOg"
      },
      "source": [
        "# Unit 3: Deep Q-Learning with Atari Games üëæ using RL Baselines3 Zoo\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/thumbnail.jpg\" alt=\"Unit 3 Thumbnail\">\n",
        "\n",
        "In this notebook, **you'll train a Deep Q-Learning agent** playing Space Invaders using [RL Baselines3 Zoo](https://github.com/DLR-RM/rl-baselines3-zoo), a training framework based on [Stable-Baselines3](https://stable-baselines3.readthedocs.io/en/master/) that provides scripts for training, evaluating agents, tuning hyperparameters, plotting results and recording videos.\n",
        "\n",
        "We're using the [RL-Baselines-3 Zoo integration, a vanilla version of Deep Q-Learning](https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html) with no extensions such as Double-DQN, Dueling-DQN, and Prioritized Experience Replay.\n",
        "\n",
        "‚¨áÔ∏è Here is an example of what **you will achieve** ‚¨áÔ∏è"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "J9S713biXntc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "db86e4f5-a278-48f0-de36-2e9cf77a037c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<video controls autoplay><source src=\"https://huggingface.co/ThomasSimonini/ppo-SpaceInvadersNoFrameskip-v4/resolve/main/replay.mp4\" type=\"video/mp4\"></video>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "%%html\n",
        "<video controls autoplay><source src=\"https://huggingface.co/ThomasSimonini/ppo-SpaceInvadersNoFrameskip-v4/resolve/main/replay.mp4\" type=\"video/mp4\"></video>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üéÆ Environments:\n",
        "\n",
        "- [SpacesInvadersNoFrameskip-v4](https://gymnasium.farama.org/environments/atari/space_invaders/)\n",
        "\n",
        "You can see the difference between Space Invaders versions here üëâ https://gymnasium.farama.org/environments/atari/space_invaders/#variants\n",
        "\n",
        "### üìö RL-Library:\n",
        "\n",
        "- [RL-Baselines3-Zoo](https://github.com/DLR-RM/rl-baselines3-zoo)"
      ],
      "metadata": {
        "id": "ykJiGevCMVc5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wciHGjrFYz9m"
      },
      "source": [
        "## Objectives of this notebook üèÜ\n",
        "At the end of the notebook, you will:\n",
        "- Be able to understand deeper **how RL Baselines3 Zoo works**.\n",
        "- Be able to **push your trained agent and the code to the Hub** with a nice video replay and an evaluation score üî•.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## This notebook is from Deep Reinforcement Learning Course\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/deep-rl-course-illustration.jpg\" alt=\"Deep RL Course illustration\"/>"
      ],
      "metadata": {
        "id": "TsnP0rjxMn1e"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nw6fJHIAZd-J"
      },
      "source": [
        "In this free course, you will:\n",
        "\n",
        "- üìñ Study Deep Reinforcement Learning in **theory and practice**.\n",
        "- üßë‚Äçüíª Learn to **use famous Deep RL libraries** such as Stable Baselines3, RL Baselines3 Zoo, CleanRL and Sample Factory 2.0.\n",
        "- ü§ñ Train **agents in unique environments**\n",
        "\n",
        "And more check üìö the syllabus üëâ https://simoninithomas.github.io/deep-rl-course\n",
        "\n",
        "Don‚Äôt forget to **<a href=\"http://eepurl.com/ic5ZUD\">sign up to the course</a>** (we are collecting your email to be able to¬†**send you the links when each Unit is published and give you information about the challenges and updates).**\n",
        "\n",
        "\n",
        "The best way to keep in touch is to join our discord server to exchange with the community and with us üëâüèª https://discord.gg/ydHrjt3WP5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vgANIBBZg1p"
      },
      "source": [
        "## Prerequisites üèóÔ∏è\n",
        "Before diving into the notebook, you need to:\n",
        "\n",
        "üî≤ üìö **[Study Deep Q-Learning by reading Unit 3](https://huggingface.co/deep-rl-course/unit3/introduction)**  ü§ó"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're constantly trying to improve our tutorials, so **if you find some issues in this notebook**, please [open an issue on the Github Repo](https://github.com/huggingface/deep-rl-class/issues)."
      ],
      "metadata": {
        "id": "7kszpGFaRVhq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QR0jZtYreSI5"
      },
      "source": [
        "# Let's train a Deep Q-Learning agent playing Atari' Space Invaders üëæ and upload it to the Hub.\n",
        "\n",
        "We strongly recommend students **to use Google Colab for the hands-on exercises instead of running them on their personal computers**.\n",
        "\n",
        "By using Google Colab, **you can focus on learning and experimenting without worrying about the technical aspects of setting up your environments**.\n",
        "\n",
        "To validate this hands-on for the certification process, you need to push your trained model to the Hub and **get a result of >= 200**.\n",
        "\n",
        "To find your result, go to the leaderboard and find your model, **the result = mean_reward - std of reward**\n",
        "\n",
        "For more information about the certification process, check this section üëâ https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## An advice üí°\n",
        "It's better to run this colab in a copy on your Google Drive, so that **if it timeouts** you still have the saved notebook on your Google Drive and do not need to fill everything from scratch.\n",
        "\n",
        "To do that you can either do `Ctrl + S` or `File > Save a copy in Google Drive.`\n",
        "\n",
        "Also, we're going to **train it for 90 minutes with 1M timesteps**. By typing `!nvidia-smi` will tell you what GPU you're using.\n",
        "\n",
        "And if you want to train more such 10 million steps, this will take about 9 hours, potentially resulting in Colab timing out. In that case, I recommend running this on your local computer (or somewhere else). Just click on: `File>Download`."
      ],
      "metadata": {
        "id": "Nc8BnyVEc3Ys"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set the GPU üí™\n",
        "- To **accelerate the agent's training, we'll use a GPU**. To do that, go to `Runtime > Change Runtime type`\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/gpu-step1.jpg\" alt=\"GPU Step 1\">"
      ],
      "metadata": {
        "id": "PU4FVzaoM6fC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- `Hardware Accelerator > GPU`\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/gpu-step2.jpg\" alt=\"GPU Step 2\">"
      ],
      "metadata": {
        "id": "KV0NyFdQM9ZG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install RL-Baselines3 Zoo and its dependencies üìö\n",
        "\n",
        "If you see `ERROR: pip's dependency resolver does not currently take into account all the packages that are installed.` **this is normal and it's not a critical error** there's a conflict of version. But the packages we need are installed."
      ],
      "metadata": {
        "id": "wS_cVefO-aYg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q git+https://github.com/DLR-RM/rl-baselines3-zoo"
      ],
      "metadata": {
        "id": "S1A_E4z3awa_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e72be5b-d42a-4e91-c139-ec7d3b4d50b3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install swig cmake ffmpeg"
      ],
      "metadata": {
        "id": "8_MllY6Om1eI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8d78291-6a5d-4da2-eb6b-ff26d4fd4527"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "cmake is already the newest version (3.22.1-1ubuntu1.22.04.2).\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "The following additional packages will be installed:\n",
            "  swig4.0\n",
            "Suggested packages:\n",
            "  swig-doc swig-examples swig4.0-examples swig4.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  swig swig4.0\n",
            "0 upgraded, 2 newly installed, 0 to remove and 18 not upgraded.\n",
            "Need to get 1,116 kB of archives.\n",
            "After this operation, 5,542 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig4.0 amd64 4.0.2-1ubuntu1 [1,110 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig all 4.0.2-1ubuntu1 [5,632 B]\n",
            "Fetched 1,116 kB in 1s (770 kB/s)\n",
            "Selecting previously unselected package swig4.0.\n",
            "(Reading database ... 124926 files and directories currently installed.)\n",
            "Preparing to unpack .../swig4.0_4.0.2-1ubuntu1_amd64.deb ...\n",
            "Unpacking swig4.0 (4.0.2-1ubuntu1) ...\n",
            "Selecting previously unselected package swig.\n",
            "Preparing to unpack .../swig_4.0.2-1ubuntu1_all.deb ...\n",
            "Unpacking swig (4.0.2-1ubuntu1) ...\n",
            "Setting up swig4.0 (4.0.2-1ubuntu1) ...\n",
            "Setting up swig (4.0.2-1ubuntu1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4S9mJiKg6SqC"
      },
      "source": [
        "To be able to use Atari games in Gymnasium we need to install atari package. And accept-rom-license to download the rom files (games files)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium[atari]\n",
        "!pip install gymnasium[accept-rom-license]"
      ],
      "metadata": {
        "id": "NsRP-lX1_2fC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c6b0b39-d8d1-4f59-fc4c-1bb6fc2df042"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium[atari] in /usr/local/lib/python3.11/dist-packages (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[atari]) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[atari]) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[atari]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[atari]) (0.0.4)\n",
            "Requirement already satisfied: ale-py>=0.9 in /usr/local/lib/python3.11/dist-packages (from gymnasium[atari]) (0.10.1)\n",
            "Requirement already satisfied: gymnasium[accept-rom-license] in /usr/local/lib/python3.11/dist-packages (1.0.0)\n",
            "\u001b[33mWARNING: gymnasium 1.0.0 does not provide the extra 'accept-rom-license'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[accept-rom-license]) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[accept-rom-license]) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[accept-rom-license]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[accept-rom-license]) (0.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a virtual display üîΩ\n",
        "\n",
        "During the notebook, we'll need to generate a replay video. To do so, with colab, **we need to have a virtual screen to be able to render the environment** (and thus record the frames).\n",
        "\n",
        "Hence the following cell will install the librairies and create and run a virtual screen üñ•"
      ],
      "metadata": {
        "id": "bTpYcVZVMzUI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "jV6wjQ7Be7p5"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!apt install python-opengl\n",
        "!apt install xvfb\n",
        "!pip3 install pyvirtualdisplay"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Virtual display\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "virtual_display = Display(visible=0, size=(1400, 900))\n",
        "virtual_display.start()"
      ],
      "metadata": {
        "id": "BE5JWP5rQIKf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4734f6f3-7b4b-4d50-d492-5b218acba693"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7c3708257890>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iPgzluo9z-u"
      },
      "source": [
        "## Train our Deep Q-Learning Agent to Play Space Invaders üëæ\n",
        "\n",
        "To train an agent with RL-Baselines3-Zoo, we just need to do two things:\n",
        "\n",
        "1. Create a hyperparameter config file that will contain our training hyperparameters called `dqn.yml`.\n",
        "\n",
        "This is a template example:\n",
        "\n",
        "```\n",
        "SpaceInvadersNoFrameskip-v4:\n",
        "  env_wrapper:\n",
        "    - stable_baselines3.common.atari_wrappers.AtariWrapper\n",
        "  frame_stack: 4\n",
        "  policy: 'CnnPolicy'\n",
        "  n_timesteps: !!float 1e6\n",
        "  buffer_size: 100000\n",
        "  learning_rate: !!float 1e-4\n",
        "  batch_size: 32\n",
        "  learning_starts: 100000\n",
        "  target_update_interval: 1000\n",
        "  train_freq: 4\n",
        "  gradient_steps: 1\n",
        "  exploration_fraction: 0.1\n",
        "  exploration_final_eps: 0.01\n",
        "  # If True, you need to deactivate handle_timeout_termination\n",
        "  # in the replay_buffer_kwargs\n",
        "  optimize_memory_usage: False\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VjblFSVDQOj"
      },
      "source": [
        "Here we see that:\n",
        "- We use the `Atari Wrapper` that preprocess the input (Frame reduction ,grayscale, stack 4 frames)\n",
        "- We use `CnnPolicy`, since we use Convolutional layers to process the frames\n",
        "- We train it for 10 million `n_timesteps`\n",
        "- Memory (Experience Replay) size is 100000, aka the amount of experience steps you saved to train again your agent with.\n",
        "\n",
        "üí° My advice is to **reduce the training timesteps to 1M,** which will take about 90 minutes on a P100. `!nvidia-smi` will tell you what GPU you're using. At 10 million steps, this will take about 9 hours, which could likely result in Colab timing out. I recommend running this on your local computer (or somewhere else). Just click on: `File>Download`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qTkbWrkECOJ"
      },
      "source": [
        "In terms of hyperparameters optimization, my advice is to focus on these 3 hyperparameters:\n",
        "- `learning_rate`\n",
        "- `buffer_size (Experience Memory size)`\n",
        "- `batch_size`\n",
        "\n",
        "As a good practice, you need to **check the documentation to understand what each hyperparameters does**: https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html#parameters\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hn8bRTHvERRL"
      },
      "source": [
        "2. We start the training and save the models on `logs` folder üìÅ\n",
        "\n",
        "- Define the algorithm after `--algo`, where we save the model after `-f` and where the hyperparameter config is after `-c`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m rl_zoo3.train --help"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ke6-geAzOtTN",
        "outputId": "833215a1-4466-4968-d15b-f24a0641a878"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-02-05 08:33:12.357385: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1738744392.377952    7077 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1738744392.384328    7077 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-02-05 08:33:12.406585: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "usage: train.py [-h] [--algo {a2c,ddpg,dqn,ppo,sac,td3,ars,crossq,qrdqn,tqc,trpo,ppo_lstm}]\n",
            "                [--env ENV] [-tb TENSORBOARD_LOG] [-i TRAINED_AGENT]\n",
            "                [--truncate-last-trajectory TRUNCATE_LAST_TRAJECTORY] [-n N_TIMESTEPS]\n",
            "                [--num-threads NUM_THREADS] [--log-interval LOG_INTERVAL] [--eval-freq EVAL_FREQ]\n",
            "                [--optimization-log-path OPTIMIZATION_LOG_PATH] [--eval-episodes EVAL_EPISODES]\n",
            "                [--n-eval-envs N_EVAL_ENVS] [--save-freq SAVE_FREQ] [--save-replay-buffer]\n",
            "                [-f LOG_FOLDER] [--seed SEED] [--vec-env {dummy,subproc}] [--device DEVICE]\n",
            "                [--n-trials N_TRIALS] [--max-total-trials MAX_TOTAL_TRIALS] [-optimize]\n",
            "                [--no-optim-plots] [--n-jobs N_JOBS] [--sampler {random,tpe,skopt}]\n",
            "                [--pruner {halving,median,none}] [--n-startup-trials N_STARTUP_TRIALS]\n",
            "                [--n-evaluations N_EVALUATIONS] [--storage STORAGE] [--study-name STUDY_NAME]\n",
            "                [--verbose VERBOSE] [--gym-packages GYM_PACKAGES [GYM_PACKAGES ...]]\n",
            "                [--env-kwargs ENV_KWARGS [ENV_KWARGS ...]]\n",
            "                [--eval-env-kwargs EVAL_ENV_KWARGS [EVAL_ENV_KWARGS ...]]\n",
            "                [-params HYPERPARAMS [HYPERPARAMS ...]] [-conf CONF_FILE] [-uuid] [--track]\n",
            "                [--wandb-project-name WANDB_PROJECT_NAME] [--wandb-entity WANDB_ENTITY] [-P]\n",
            "                [-tags WANDB_TAGS [WANDB_TAGS ...]]\n",
            "\n",
            "options:\n",
            "  -h, --help            show this help message and exit\n",
            "  --algo {a2c,ddpg,dqn,ppo,sac,td3,ars,crossq,qrdqn,tqc,trpo,ppo_lstm}\n",
            "                        RL Algorithm\n",
            "  --env ENV             environment ID\n",
            "  -tb TENSORBOARD_LOG, --tensorboard-log TENSORBOARD_LOG\n",
            "                        Tensorboard log dir\n",
            "  -i TRAINED_AGENT, --trained-agent TRAINED_AGENT\n",
            "                        Path to a pretrained agent to continue training\n",
            "  --truncate-last-trajectory TRUNCATE_LAST_TRAJECTORY\n",
            "                        When using HER with online sampling the last trajectory in the replay\n",
            "                        buffer will be truncated after reloading the replay buffer.\n",
            "  -n N_TIMESTEPS, --n-timesteps N_TIMESTEPS\n",
            "                        Overwrite the number of timesteps\n",
            "  --num-threads NUM_THREADS\n",
            "                        Number of threads for PyTorch (-1 to use default)\n",
            "  --log-interval LOG_INTERVAL\n",
            "                        Override log interval (default: -1, no change)\n",
            "  --eval-freq EVAL_FREQ\n",
            "                        Evaluate the agent every n steps (if negative, no evaluation). During\n",
            "                        hyperparameter optimization n-evaluations is used instead\n",
            "  --optimization-log-path OPTIMIZATION_LOG_PATH\n",
            "                        Path to save the evaluation log and optimal policy for each hyperparameter\n",
            "                        tried during optimization. Disabled if no argument is passed.\n",
            "  --eval-episodes EVAL_EPISODES\n",
            "                        Number of episodes to use for evaluation\n",
            "  --n-eval-envs N_EVAL_ENVS\n",
            "                        Number of environments for evaluation\n",
            "  --save-freq SAVE_FREQ\n",
            "                        Save the model every n steps (if negative, no checkpoint)\n",
            "  --save-replay-buffer  Save the replay buffer too (when applicable)\n",
            "  -f LOG_FOLDER, --log-folder LOG_FOLDER\n",
            "                        Log folder\n",
            "  --seed SEED           Random generator seed\n",
            "  --vec-env {dummy,subproc}\n",
            "                        VecEnv type\n",
            "  --device DEVICE       PyTorch device to be use (ex: cpu, cuda...)\n",
            "  --n-trials N_TRIALS   Number of trials for optimizing hyperparameters. This applies to each\n",
            "                        optimization runner, not the entire optimization process.\n",
            "  --max-total-trials MAX_TOTAL_TRIALS\n",
            "                        Number of (potentially pruned) trials for optimizing hyperparameters. This\n",
            "                        applies to the entire optimization process and takes precedence over\n",
            "                        --n-trials if set.\n",
            "  -optimize, --optimize-hyperparameters\n",
            "                        Run hyperparameters search\n",
            "  --no-optim-plots      Disable hyperparameter optimization plots\n",
            "  --n-jobs N_JOBS       Number of parallel jobs when optimizing hyperparameters\n",
            "  --sampler {random,tpe,skopt}\n",
            "                        Sampler to use when optimizing hyperparameters\n",
            "  --pruner {halving,median,none}\n",
            "                        Pruner to use when optimizing hyperparameters\n",
            "  --n-startup-trials N_STARTUP_TRIALS\n",
            "                        Number of trials before using optuna sampler\n",
            "  --n-evaluations N_EVALUATIONS\n",
            "                        Training policies are evaluated every n-timesteps // n-evaluations steps\n",
            "                        when doing hyperparameter optimization.Default is 1 evaluation per 100k\n",
            "                        timesteps.\n",
            "  --storage STORAGE     Database storage path if distributed optimization should be used\n",
            "  --study-name STUDY_NAME\n",
            "                        Study name for distributed optimization\n",
            "  --verbose VERBOSE     Verbose mode (0: no output, 1: INFO)\n",
            "  --gym-packages GYM_PACKAGES [GYM_PACKAGES ...]\n",
            "                        Additional external Gym environment package modules to import\n",
            "  --env-kwargs ENV_KWARGS [ENV_KWARGS ...]\n",
            "                        Optional keyword argument to pass to the env constructor\n",
            "  --eval-env-kwargs EVAL_ENV_KWARGS [EVAL_ENV_KWARGS ...]\n",
            "                        Optional keyword argument to pass to the env constructor for evaluation\n",
            "  -params HYPERPARAMS [HYPERPARAMS ...], --hyperparams HYPERPARAMS [HYPERPARAMS ...]\n",
            "                        Overwrite hyperparameter (e.g. learning_rate:0.01 train_freq:10)\n",
            "  -conf CONF_FILE, --conf-file CONF_FILE\n",
            "                        Custom yaml file or python package from which the hyperparameters will be\n",
            "                        loaded.We expect that python packages contain a dictionary called\n",
            "                        'hyperparams' which contains a key for each environment.\n",
            "  -uuid, --uuid         Ensure that the run has a unique ID\n",
            "  --track               if toggled, this experiment will be tracked with Weights and Biases\n",
            "  --wandb-project-name WANDB_PROJECT_NAME\n",
            "                        the wandb's project name\n",
            "  --wandb-entity WANDB_ENTITY\n",
            "                        the entity (team) of wandb's project\n",
            "  -P, --progress        if toggled, display a progress bar using tqdm and rich\n",
            "  -tags WANDB_TAGS [WANDB_TAGS ...], --wandb-tags WANDB_TAGS [WANDB_TAGS ...]\n",
            "                        Tags for wandb run, e.g.: -tags optimized pr-123\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "PuocgdokSab9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19a30c10-69c5-448e-9cbd-2d52cc870431"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-02-05 08:34:29.948858: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1738744469.968875    7429 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1738744469.975136    7429 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-02-05 08:34:29.995665: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "========== SpaceInvadersNoFrameskip-v4 ==========\n",
            "Seed: 3347496400\n",
            "Loading hyperparameters from: dqn.yml\n",
            "Default hyperparameters for environment (ones being tuned will be overridden):\n",
            "OrderedDict([('batch_size', 32),\n",
            "             ('buffer_size', 100000),\n",
            "             ('env_wrapper',\n",
            "              ['stable_baselines3.common.atari_wrappers.AtariWrapper']),\n",
            "             ('exploration_final_eps', 0.01),\n",
            "             ('exploration_fraction', 0.1),\n",
            "             ('frame_stack', 4),\n",
            "             ('gradient_steps', 1),\n",
            "             ('learning_rate', 0.0001),\n",
            "             ('learning_starts', 10000),\n",
            "             ('n_timesteps', 1000000.0),\n",
            "             ('optimize_memory_usage', False),\n",
            "             ('policy', 'CnnPolicy'),\n",
            "             ('target_update_interval', 1000),\n",
            "             ('train_freq', 4)])\n",
            "A.L.E: Arcade Learning Environment (version 0.10.1+unknown)\n",
            "[Powered by Stella]\n",
            "Log path: logs//dqn/SpaceInvadersNoFrameskip-v4_3\n",
            "\u001b[2KEval num_timesteps=25000, episode_reward=139.00 +/- 97.54\n",
            "\u001b[2KEpisode length: 2298.60 +/- 618.65\n",
            "\u001b[2KNew best mean reward!\n",
            "\u001b[2KEval num_timesteps=50000, episode_reward=156.00 +/- 138.61\n",
            "\u001b[2KEpisode length: 2209.80 +/- 785.71\n",
            "\u001b[2KNew best mean reward!\n",
            "\u001b[2KEval num_timesteps=75000, episode_reward=161.00 +/- 30.56\n",
            "\u001b[2KEpisode length: 2647.80 +/- 315.01\n",
            "\u001b[2KNew best mean reward!\n",
            "\u001b[2KEval num_timesteps=100000, episode_reward=128.00 +/- 22.05\n",
            "\u001b[2KEpisode length: 2125.00 +/- 238.42\n",
            "\u001b[2KEval num_timesteps=125000, episode_reward=177.00 +/- 22.05\n",
            "\u001b[2KEpisode length: 2310.20 +/- 195.80\n",
            "\u001b[2KNew best mean reward!\n",
            "\u001b[2KEval num_timesteps=150000, episode_reward=79.00 +/- 30.56\n",
            "\u001b[2KEpisode length: 2031.80 +/- 247.92\n",
            "\u001b[2KEval num_timesteps=175000, episode_reward=222.00 +/- 22.05\n",
            "\u001b[2KEpisode length: 2299.40 +/- 678.71\n",
            "\u001b[2KNew best mean reward!\n",
            "\u001b[2KEval num_timesteps=200000, episode_reward=340.00 +/- 138.82\n",
            "\u001b[2KEpisode length: 2827.00 +/- 532.70\n",
            "\u001b[2KNew best mean reward!\n",
            "\u001b[2KEval num_timesteps=225000, episode_reward=299.00 +/- 180.40\n",
            "\u001b[2KEpisode length: 2381.80 +/- 461.08\n",
            "\u001b[2KEval num_timesteps=250000, episode_reward=211.00 +/- 57.57\n",
            "\u001b[2KEpisode length: 2785.00 +/- 346.61\n",
            "\u001b[2KEval num_timesteps=275000, episode_reward=121.00 +/- 27.46\n",
            "\u001b[2KEpisode length: 3159.40 +/- 472.94\n",
            "\u001b[2KEval num_timesteps=300000, episode_reward=235.00 +/- 145.71\n",
            "\u001b[2KEpisode length: 2284.60 +/- 371.01\n",
            "\u001b[2KEval num_timesteps=325000, episode_reward=370.00 +/- 118.15\n",
            "\u001b[2KEpisode length: 2614.60 +/- 675.62\n",
            "\u001b[2KNew best mean reward!\n",
            "\u001b[2KEval num_timesteps=350000, episode_reward=535.00 +/- 169.14\n",
            "\u001b[2KEpisode length: 3411.00 +/- 479.47\n",
            "\u001b[2KNew best mean reward!\n",
            "\u001b[2KEval num_timesteps=375000, episode_reward=293.00 +/- 92.66\n",
            "\u001b[2KEpisode length: 3376.60 +/- 372.30\n",
            "\u001b[2KEval num_timesteps=400000, episode_reward=247.00 +/- 136.70\n",
            "\u001b[2KEpisode length: 2479.80 +/- 807.52\n",
            "\u001b[2KEval num_timesteps=425000, episode_reward=269.00 +/- 32.00\n",
            "\u001b[2KEpisode length: 2748.20 +/- 476.51\n",
            "\u001b[2KEval num_timesteps=450000, episode_reward=369.00 +/- 155.29\n",
            "\u001b[2KEpisode length: 3144.60 +/- 916.96\n",
            "\u001b[2KEval num_timesteps=475000, episode_reward=448.00 +/- 109.39\n",
            "\u001b[2KEpisode length: 3385.80 +/- 525.55\n",
            "\u001b[2KEval num_timesteps=500000, episode_reward=559.00 +/- 288.21\n",
            "\u001b[2KEpisode length: 3207.40 +/- 1535.88\n",
            "\u001b[2KNew best mean reward!\n",
            "\u001b[2KEval num_timesteps=525000, episode_reward=528.00 +/- 101.02\n",
            "\u001b[2KEpisode length: 3306.60 +/- 472.97\n",
            "\u001b[2KEval num_timesteps=550000, episode_reward=490.00 +/- 104.55\n",
            "\u001b[2KEpisode length: 3731.00 +/- 518.53\n",
            "\u001b[2KEval num_timesteps=575000, episode_reward=372.00 +/- 105.39\n",
            "\u001b[2KEpisode length: 3158.60 +/- 535.81\n",
            "\u001b[2KEval num_timesteps=600000, episode_reward=488.00 +/- 81.22\n",
            "\u001b[2KEpisode length: 3963.60 +/- 278.93\n",
            "\u001b[2KEval num_timesteps=625000, episode_reward=628.00 +/- 75.87\n",
            "\u001b[2KEpisode length: 4165.80 +/- 279.71\n",
            "\u001b[2KNew best mean reward!\n",
            "\u001b[2KEval num_timesteps=650000, episode_reward=574.00 +/- 166.93\n",
            "\u001b[2KEpisode length: 4039.40 +/- 760.27\n",
            "\u001b[2KEval num_timesteps=675000, episode_reward=437.00 +/- 113.60\n",
            "\u001b[2KEpisode length: 3063.80 +/- 595.63\n",
            "\u001b[2KEval num_timesteps=700000, episode_reward=361.00 +/- 165.33\n",
            "\u001b[2KEpisode length: 3035.80 +/- 1084.18\n",
            "\u001b[2KEval num_timesteps=725000, episode_reward=390.00 +/- 38.86\n",
            "\u001b[2KEpisode length: 3609.40 +/- 534.37\n",
            "\u001b[2KEval num_timesteps=750000, episode_reward=572.00 +/- 122.74\n",
            "\u001b[2KEpisode length: 4331.40 +/- 343.17\n",
            "\u001b[2KEval num_timesteps=775000, episode_reward=500.00 +/- 84.56\n",
            "\u001b[2KEpisode length: 3524.20 +/- 541.38\n",
            "\u001b[2KEval num_timesteps=800000, episode_reward=526.00 +/- 129.05\n",
            "\u001b[2KEpisode length: 4323.80 +/- 190.21\n",
            "\u001b[2KEval num_timesteps=825000, episode_reward=612.00 +/- 134.48\n",
            "\u001b[2KEpisode length: 4101.00 +/- 390.05\n",
            "\u001b[2KEval num_timesteps=850000, episode_reward=696.00 +/- 259.49\n",
            "\u001b[2KEpisode length: 4387.40 +/- 1235.76\n",
            "\u001b[2KNew best mean reward!\n",
            "\u001b[2KEval num_timesteps=875000, episode_reward=607.00 +/- 199.41\n",
            "\u001b[2KEpisode length: 4161.00 +/- 1450.18\n",
            "\u001b[2KEval num_timesteps=900000, episode_reward=691.00 +/- 223.15\n",
            "\u001b[2KEpisode length: 4453.60 +/- 1127.29\n",
            "\u001b[2KEval num_timesteps=925000, episode_reward=514.00 +/- 112.40\n",
            "\u001b[2KEpisode length: 4194.40 +/- 682.39\n",
            "\u001b[2KEval num_timesteps=950000, episode_reward=647.00 +/- 224.78\n",
            "\u001b[2KEpisode length: 3573.80 +/- 645.75\n",
            "\u001b[2KEval num_timesteps=975000, episode_reward=688.00 +/- 105.86\n",
            "\u001b[2KEpisode length: 3970.60 +/- 349.59\n",
            "\u001b[2KEval num_timesteps=1000000, episode_reward=487.00 +/- 51.24\n",
            "\u001b[2KEpisode length: 4615.80 +/- 408.28\n",
            "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1,000,000/1,000,000 \u001b[0m [ \u001b[33m1:11:40\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m124 it/s\u001b[0m ]\n",
            "\u001b[?25hSaving to logs//dqn/SpaceInvadersNoFrameskip-v4_3\n"
          ]
        }
      ],
      "source": [
        "!python -m rl_zoo3.train --algo dqn --env SpaceInvadersNoFrameskip-v4 -f logs/ -c dqn.yml --verbose 0 -P"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dLomIiMKQaf"
      },
      "source": [
        "## Let's evaluate our agent üëÄ\n",
        "- RL-Baselines3-Zoo provides `enjoy.py`, a python script to evaluate our agent. In most RL libraries, we call the evaluation script `enjoy.py`.\n",
        "- Let's evaluate it for 5000 timesteps üî•"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "P_uSmwGRSk0z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe447b27-0fea-49ce-b61f-19c0a81094ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-02-05 09:46:39.506490: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1738748799.541697   25336 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1738748799.552373   25336 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-02-05 09:46:39.586536: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Loading latest experiment, id=3\n",
            "Loading logs/dqn/SpaceInvadersNoFrameskip-v4_3/SpaceInvadersNoFrameskip-v4.zip\n",
            "A.L.E: Arcade Learning Environment (version 0.10.1+unknown)\n",
            "[Powered by Stella]\n",
            "Stacking 4 frames\n",
            "Atari Episode Score: 605.00\n",
            "Atari Episode Length 4861\n",
            "Atari Episode Score: 610.00\n",
            "Atari Episode Length 3681\n",
            "Atari Episode Score: 610.00\n",
            "Atari Episode Length 3941\n",
            "Atari Episode Score: 435.00\n",
            "Atari Episode Length 3677\n",
            "Atari Episode Score: 770.00\n",
            "Atari Episode Length 3841\n"
          ]
        }
      ],
      "source": [
        "!python -m rl_zoo3.enjoy  --algo dqn  --env SpaceInvadersNoFrameskip-v4  --no-render  --n-timesteps 5000  --folder logs/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liBeTltiHJtr"
      },
      "source": [
        "## Publish our trained model on the Hub üöÄ\n",
        "Now that we saw we got good results after the training, we can publish our trained model on the hub ü§ó with one line of code.\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit3/space-invaders-model.gif\" alt=\"Space Invaders model\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezbHS1q3HYVV"
      },
      "source": [
        "By using `rl_zoo3.push_to_hub` **you evaluate, record a replay, generate a model card of your agent and push it to the hub**.\n",
        "\n",
        "This way:\n",
        "- You can **showcase our work** üî•\n",
        "- You can **visualize your agent playing** üëÄ\n",
        "- You can **share with the community an agent that others can use** üíæ\n",
        "- You can **access a leaderboard üèÜ to see how well your agent is performing compared to your classmates** üëâ  https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMSeZRBiHk6X"
      },
      "source": [
        "To be able to share your model with the community there are three more steps to follow:\n",
        "\n",
        "1Ô∏è‚É£ (If it's not already done) create an account to HF ‚û° https://huggingface.co/join\n",
        "\n",
        "2Ô∏è‚É£ Sign in and then, you need to store your authentication token from the Hugging Face website.\n",
        "- Create a new token (https://huggingface.co/settings/tokens) **with write role**\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/create-token.jpg\" alt=\"Create HF Token\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9O6FI0F8HnzE"
      },
      "source": [
        "- Copy the token\n",
        "- Run the cell below and past the token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Ppu9yePwHrZX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "82d0388f542944c9bd2988762d0338ff",
            "fa8f449b3e3e4625be447d5cc465a531",
            "99bc124dfb3e497d8be7fc2a2a05b2f7",
            "37a8ba38b3354d0db6e5da0c4275e335",
            "adb8693d09b843499a3fda022472fd85",
            "bf53be9a12194560ab9a66cd5a18e475",
            "25d3a234456c4d8fa04c651b85617fb7",
            "719d38228cf94e23b1b21f5dbe8b7388",
            "e769eabd38ab4cae97441b0c985a2e7c",
            "fa3868ba1f5841458f6377834831ae10",
            "2ee61344e3e74f92bc905ba513aa125f",
            "69ecc8107b3b4c359c8bf6055ffda498",
            "30c75aaf6a3b449cbee496c7faf14d46",
            "d47fe9028c054a35818cf8fb72ad35e9",
            "fa97d122d67347eca650b1b2d6ca7d95",
            "e8151f1d691645cb816b1dbd22e271ae",
            "19c0df97857d42dbb21ed168bb8ca24e",
            "00a94e7d719b4afdac9bfed8aabb54f4",
            "89cf34eb0d0e4916a11bf40fb6d7d688",
            "fd2e64956f604409a606302a4ea60f65"
          ]
        },
        "outputId": "19823e7c-6237-4a24-e193-4433f21e5270"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "82d0388f542944c9bd2988762d0338ff"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login # To log to our Hugging Face account to be able to upload models to the Hub.\n",
        "notebook_login()\n",
        "!git config --global credential.helper store"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RVEdunPHs8B"
      },
      "source": [
        "If you don't want to use a Google Colab or a Jupyter Notebook, you need to use this command instead: `huggingface-cli login`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSLwdmvhHvjw"
      },
      "source": [
        "3Ô∏è‚É£ We're now ready to push our trained agent to the ü§ó Hub üî•"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PW436XnhHw1H"
      },
      "source": [
        "Let's run push_to_hub.py file to upload our trained agent to the Hub.\n",
        "\n",
        "`--repo-name `: The name of the repo\n",
        "\n",
        "`-orga`: Your Hugging Face username\n",
        "\n",
        "`-f`: Where the trained model folder is (in our case `logs`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "_HQNlAXuEhci",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bf2c992-50f1-485d-de1e-4ffdf6e39ca9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-02-05 09:49:18.056678: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1738748958.077631   26022 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1738748958.084014   26022 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-02-05 09:49:18.106048: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Loading latest experiment, id=3\n",
            "Loading logs/dqn/SpaceInvadersNoFrameskip-v4_3/SpaceInvadersNoFrameskip-v4.zip\n",
            "A.L.E: Arcade Learning Environment (version 0.10.1+unknown)\n",
            "[Powered by Stella]\n",
            "Stacking 4 frames\n",
            "Uploading to pankajr141/huggingface_deeprl_unit3_SpaceInvadersNoFrameskip-v4, make sure to have the rights\n",
            "\u001b[38;5;4m‚Ñπ This function will save, evaluate, generate a video of your agent,\n",
            "create a model card and push everything to the hub. It might take up to some\n",
            "minutes if video generation is activated. This is a work in progress: if you\n",
            "encounter a bug, please open an issue.\u001b[0m\n",
            "Fetching 1 files:   0% 0/1 [00:00<?, ?it/s]\n",
            ".gitattributes: 100% 1.52k/1.52k [00:00<00:00, 10.9MB/s]\n",
            "Fetching 1 files: 100% 1/1 [00:00<00:00,  4.14it/s]\n",
            "Saving model to: hub/huggingface_deeprl_unit3_SpaceInvadersNoFrameskip-v4/dqn-SpaceInvadersNoFrameskip-v4\n",
            "Saving video to /tmp/tmp0evvio8r/-step-0-to-step-1000.mp4\n",
            "/usr/local/lib/python3.11/dist-packages/moviepy/config_defaults.py:1: DeprecationWarning: invalid escape sequence '\\P'\n",
            "  \"\"\"\n",
            "Moviepy - Building video /tmp/tmp0evvio8r/-step-0-to-step-1000.mp4.\n",
            "Moviepy - Writing video /tmp/tmp0evvio8r/-step-0-to-step-1000.mp4\n",
            "\n",
            "Moviepy - Done !\n",
            "Moviepy - video ready /tmp/tmp0evvio8r/-step-0-to-step-1000.mp4\n",
            "\u001b[38;5;1m‚úò 'DummyVecEnv' object has no attribute 'video_recorder'\u001b[0m\n",
            "\u001b[38;5;1m‚úò We are unable to generate a replay of your agent, the package_to_hub\n",
            "process continues\u001b[0m\n",
            "\u001b[38;5;1m‚úò Please open an issue at\n",
            "https://github.com/huggingface/huggingface_sb3/issues\u001b[0m\n",
            "\u001b[38;5;4m‚Ñπ Pushing repo huggingface_deeprl_unit3_SpaceInvadersNoFrameskip-v4 to\n",
            "the Hugging Face Hub\u001b[0m\n",
            "policy.optimizer.pth:   0% 0.00/13.5M [00:00<?, ?B/s]\n",
            "policy.pth:   0% 0.00/13.5M [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "pytorch_variables.pth:   0% 0.00/864 [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "dqn-SpaceInvadersNoFrameskip-v4.zip:   0% 0.00/27.2M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Upload 5 LFS files:   0% 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "train_eval_metrics.zip:   0% 0.00/34.7k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "policy.pth:  10% 1.38M/13.5M [00:00<00:01, 12.0MB/s]\u001b[A\n",
            "\n",
            "\n",
            "pytorch_variables.pth: 100% 864/864 [00:00<00:00, 6.03kB/s]\n",
            "train_eval_metrics.zip: 100% 34.7k/34.7k [00:00<00:00, 195kB/s]\n",
            "policy.optimizer.pth:  63% 8.52M/13.5M [00:00<00:00, 35.2MB/s]\n",
            "policy.pth:  34% 4.55M/13.5M [00:00<00:00, 18.4MB/s]\u001b[A\n",
            "policy.pth:  60% 8.04M/13.5M [00:00<00:00, 24.0MB/s]\u001b[A\n",
            "\n",
            "\n",
            "policy.optimizer.pth:  89% 12.0M/13.5M [00:00<00:00, 23.4MB/s]\n",
            "\n",
            "\n",
            "dqn-SpaceInvadersNoFrameskip-v4.zip:  54% 14.7M/27.2M [00:00<00:00, 31.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "policy.pth:  82% 11.0M/13.5M [00:00<00:00, 18.6MB/s]\u001b[A\n",
            "\n",
            "\n",
            "policy.optimizer.pth: 100% 13.5M/13.5M [00:00<00:00, 16.5MB/s]\n",
            "dqn-SpaceInvadersNoFrameskip-v4.zip: 100% 27.2M/27.2M [00:00<00:00, 27.9MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "policy.pth: 100% 13.5M/13.5M [00:01<00:00, 7.56MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Upload 5 LFS files:  40% 2/5 [00:02<00:03,  1.01s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Upload 5 LFS files: 100% 5/5 [00:04<00:00,  1.15it/s]\n",
            "\u001b[38;5;4m‚Ñπ Your model is pushed to the hub. You can view your model here:\n",
            "https://huggingface.co/pankajr141/huggingface_deeprl_unit3_SpaceInvadersNoFrameskip-v4\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python -m rl_zoo3.push_to_hub  --algo dqn  --env SpaceInvadersNoFrameskip-v4  --repo-name huggingface_deeprl_unit3_SpaceInvadersNoFrameskip-v4  -orga pankajr141  -f logs/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0D4F5zsTTJ-L"
      },
      "source": [
        "###."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff89kd2HL1_s"
      },
      "source": [
        "Congrats ü•≥ you've just trained and uploaded your first Deep Q-Learning agent using RL-Baselines-3 Zoo. The script above should have displayed a link to a model repository such as https://huggingface.co/ThomasSimonini/dqn-SpaceInvadersNoFrameskip-v4. When you go to this link, you can:\n",
        "\n",
        "- See a **video preview of your agent** at the right.\n",
        "- Click \"Files and versions\" to see all the files in the repository.\n",
        "- Click \"Use in stable-baselines3\" to get a code snippet that shows how to load the model.\n",
        "- A model card (`README.md` file) which gives a description of the model and the hyperparameters you used.\n",
        "\n",
        "Under the hood, the Hub uses git-based repositories (don't worry if you don't know what git is), which means you can update the model with new versions as you experiment and improve your agent.\n",
        "\n",
        "**Compare the results of your agents with your classmates** using the [leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard) üèÜ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyRKcCYY-dIo"
      },
      "source": [
        "## Load a powerful trained model üî•\n",
        "- The Stable-Baselines3 team uploaded **more than 150 trained Deep Reinforcement Learning agents on the Hub**.\n",
        "\n",
        "You can find them here: üëâ https://huggingface.co/sb3\n",
        "\n",
        "Some examples:\n",
        "- Asteroids: https://huggingface.co/sb3/dqn-AsteroidsNoFrameskip-v4\n",
        "- Beam Rider: https://huggingface.co/sb3/dqn-BeamRiderNoFrameskip-v4\n",
        "- Breakout: https://huggingface.co/sb3/dqn-BreakoutNoFrameskip-v4\n",
        "- Road Runner: https://huggingface.co/sb3/dqn-RoadRunnerNoFrameskip-v4\n",
        "\n",
        "Let's load an agent playing Beam Rider: https://huggingface.co/sb3/dqn-BeamRiderNoFrameskip-v4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "B-9QVFIROI5Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "decc867c-3439-4839-96a1-1c4f3befd917"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<video controls autoplay><source src=\"https://huggingface.co/sb3/dqn-BeamRiderNoFrameskip-v4/resolve/main/replay.mp4\" type=\"video/mp4\"></video>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "%%html\n",
        "<video controls autoplay><source src=\"https://huggingface.co/sb3/dqn-BeamRiderNoFrameskip-v4/resolve/main/replay.mp4\" type=\"video/mp4\"></video>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZQNY_r6NJtC"
      },
      "source": [
        "1. We download the model using `rl_zoo3.load_from_hub`, and place it in a new folder that we can call `rl_trained`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "OdBNZHy0NGTR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85718075-fc8f-4a81-a765-1cf73968ecdf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-02-05 09:51:01.501323: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1738749061.521689   26484 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1738749061.528011   26484 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-02-05 09:51:01.550052: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Downloading from https://huggingface.co/sb3/dqn-BeamRiderNoFrameskip-v4\n",
            "dqn-BeamRiderNoFrameskip-v4.zip: 100% 27.2M/27.2M [00:00<00:00, 169MB/s]\n",
            "config.yml: 100% 548/548 [00:00<00:00, 3.39MB/s]\n",
            "No normalization file\n",
            "args.yml: 100% 887/887 [00:00<00:00, 4.46MB/s]\n",
            "env_kwargs.yml: 100% 3.00/3.00 [00:00<00:00, 18.4kB/s]\n",
            "train_eval_metrics.zip: 100% 244k/244k [00:00<00:00, 293MB/s]\n",
            "Saving to rl_trained/dqn/BeamRiderNoFrameskip-v4_1\n"
          ]
        }
      ],
      "source": [
        "# Download model and save it into the logs/ folder\n",
        "!python -m rl_zoo3.load_from_hub --algo dqn --env BeamRiderNoFrameskip-v4 -orga sb3 -f rl_trained/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFt6hmWsNdBo"
      },
      "source": [
        "2. Let's evaluate if for 5000 timesteps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "aOxs0rNuN0uS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "859dbe66-d26a-4518-e466-ebf2f8d0e140"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-02-05 09:51:18.058614: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1738749078.080031   26567 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1738749078.086370   26567 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-02-05 09:51:18.106629: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Loading latest experiment, id=1\n",
            "Loading rl_trained/dqn/BeamRiderNoFrameskip-v4_1/BeamRiderNoFrameskip-v4.zip\n",
            "A.L.E: Arcade Learning Environment (version 0.10.1+unknown)\n",
            "[Powered by Stella]\n",
            "Stacking 4 frames\n",
            "/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/save_util.py:167: UserWarning: Could not deserialize object exploration_schedule. Consider using `custom_objects` argument to replace this object.\n",
            "Exception: code expected at least 16 arguments, got 15\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/vec_env/patch_gym.py:95: UserWarning: You loaded a model that was trained using OpenAI Gym. We strongly recommend transitioning to Gymnasium by saving that model again.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/base_class.py:773: UserWarning: You are probably loading a DQN model saved with SB3 < 2.4.0, we truncated the optimizer state so you can save the model again to avoid issues in the future (see https://github.com/DLR-RM/stable-baselines3/pull/1963 for more info). Original error: loaded state dict contains a parameter group that doesn't match the size of optimizer's group \n",
            "Note: the model should still work fine, this only a warning.\n",
            "  warnings.warn(\n",
            "Atari Episode Score: 3028.00\n",
            "Atari Episode Length 14816\n"
          ]
        }
      ],
      "source": [
        "!python -m rl_zoo3.enjoy --algo dqn --env BeamRiderNoFrameskip-v4 -n 5000  -f rl_trained/ --no-render"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxMDuDfPON57"
      },
      "source": [
        "Why not trying to train your own **Deep Q-Learning Agent playing BeamRiderNoFrameskip-v4? üèÜ.**\n",
        "\n",
        "If you want to try, check https://huggingface.co/sb3/dqn-BeamRiderNoFrameskip-v4#hyperparameters **in the model card, you have the hyperparameters of the trained agent.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xL_ZtUgpOuY6"
      },
      "source": [
        "But finding hyperparameters can be a daunting task. Fortunately, we'll see in the next Unit, how we can **use Optuna for optimizing the Hyperparameters üî•.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pqaco8W-huW"
      },
      "source": [
        "## Some additional challenges üèÜ\n",
        "The best way to learn **is to try things by your own**!\n",
        "\n",
        "In the [Leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard) you will find your agents. Can you get to the top?\n",
        "\n",
        "Here's a list of environments you can try to train your agent with:\n",
        "- BeamRiderNoFrameskip-v4\n",
        "- BreakoutNoFrameskip-v4\n",
        "- EnduroNoFrameskip-v4\n",
        "- PongNoFrameskip-v4\n",
        "\n",
        "Also, **if you want to learn to implement Deep Q-Learning by yourself**, you definitely should look at CleanRL implementation: https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/dqn_atari.py\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/atari-envs.gif\" alt=\"Environments\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paS-XKo4-kmu"
      },
      "source": [
        "________________________________________________________________________\n",
        "Congrats on finishing this chapter!\n",
        "\n",
        "If you‚Äôre still feel confused with all these elements...it's totally normal! **This was the same for me and for all people who studied RL.**\n",
        "\n",
        "Take time to really **grasp the material before continuing and try the additional challenges**. It‚Äôs important to master these elements and having a solid foundations.\n",
        "\n",
        "In the next unit, **we‚Äôre going to learn about [Optuna](https://optuna.org/)**. One of the most critical task in Deep Reinforcement Learning is to find a good set of training hyperparameters. And Optuna is a library that helps you to automate the search.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WRx7tO7-mvC"
      },
      "source": [
        "\n",
        "\n",
        "### This is a course built with you üë∑üèø‚Äç‚ôÄÔ∏è\n",
        "\n",
        "Finally, we want to improve and update the course iteratively with your feedback. If you have some, please fill this form üëâ https://forms.gle/3HgA7bEHwAmmLfwh9\n",
        "\n",
        "We're constantly trying to improve our tutorials, so **if you find some issues in this notebook**, please [open an issue on the Github Repo](https://github.com/huggingface/deep-rl-class/issues)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "See you on Bonus unit 2! üî•"
      ],
      "metadata": {
        "id": "Kc3udPT-RcXc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fS3Xerx0fIMV"
      },
      "source": [
        "### Keep Learning, Stay Awesome ü§ó"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "82d0388f542944c9bd2988762d0338ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_25d3a234456c4d8fa04c651b85617fb7"
          }
        },
        "fa8f449b3e3e4625be447d5cc465a531": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_719d38228cf94e23b1b21f5dbe8b7388",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_e769eabd38ab4cae97441b0c985a2e7c",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "99bc124dfb3e497d8be7fc2a2a05b2f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_fa3868ba1f5841458f6377834831ae10",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_2ee61344e3e74f92bc905ba513aa125f",
            "value": ""
          }
        },
        "37a8ba38b3354d0db6e5da0c4275e335": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_69ecc8107b3b4c359c8bf6055ffda498",
            "style": "IPY_MODEL_30c75aaf6a3b449cbee496c7faf14d46",
            "value": true
          }
        },
        "adb8693d09b843499a3fda022472fd85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_d47fe9028c054a35818cf8fb72ad35e9",
            "style": "IPY_MODEL_fa97d122d67347eca650b1b2d6ca7d95",
            "tooltip": ""
          }
        },
        "bf53be9a12194560ab9a66cd5a18e475": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e8151f1d691645cb816b1dbd22e271ae",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_19c0df97857d42dbb21ed168bb8ca24e",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "25d3a234456c4d8fa04c651b85617fb7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "719d38228cf94e23b1b21f5dbe8b7388": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e769eabd38ab4cae97441b0c985a2e7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fa3868ba1f5841458f6377834831ae10": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ee61344e3e74f92bc905ba513aa125f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "69ecc8107b3b4c359c8bf6055ffda498": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30c75aaf6a3b449cbee496c7faf14d46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d47fe9028c054a35818cf8fb72ad35e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa97d122d67347eca650b1b2d6ca7d95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "e8151f1d691645cb816b1dbd22e271ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19c0df97857d42dbb21ed168bb8ca24e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "00a94e7d719b4afdac9bfed8aabb54f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_89cf34eb0d0e4916a11bf40fb6d7d688",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_fd2e64956f604409a606302a4ea60f65",
            "value": "Connecting..."
          }
        },
        "89cf34eb0d0e4916a11bf40fb6d7d688": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd2e64956f604409a606302a4ea60f65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}